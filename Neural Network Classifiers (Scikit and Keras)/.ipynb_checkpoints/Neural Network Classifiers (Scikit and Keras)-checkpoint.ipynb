{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Neural Network Classifiers (Scikit and Keras)\n",
    "# Author: Robert Zacchigna\n",
    "\n",
    "* [Dataset - Categorical Comment Texts](#Dataset)\n",
    "    * [Columns](#Columns)\n",
    "* [Imports](#Imports)\n",
    "* [Classes](#Classes)\n",
    "\n",
    "\n",
    "* [Load and Clean Comment Data](#Clean_Data)\n",
    "    * [Clean Text Data: Lower, Remove Punctuation, Remove Stopwords and Stem Words](#Clean_Text)\n",
    "    * [Split Data Into Train and Test Sets](#Split_Dataset)\n",
    "        * [Training Set: Feature Data](#Train_Set)\n",
    "        * [Training Set: Target Data](#Test_Set)\n",
    "\n",
    "\n",
    "* **[Part 1: Neural Network Classifier with Scikit](#Part_1)**\n",
    "    * [Create Model Classification Pipeline with TfidfVectorizer and MLPClassifier (Neural Net)](#Part_1-Scikit_Pipe)\n",
    "\t* [Setup Parameter Grid and Fit Pipeline and Parameters to GridSearchCV](#Part_1-Scikit_Grid)\n",
    "\t* [Fit Training Data to Classification Grid to Find the Best Parameters for the Model](#Part_1-Scikit_Fit_Grid)\n",
    "\t* [Display Top Model Scores Found by GridSearchCV](#Part_1-Scikit_Top_Scores)\n",
    "\t* [Best Score Found From Classification Grid](#Part_1-Scikit_Best_Scores)\n",
    "\t* [Best Parameters Found from Classification Grid](#Part_1-Scikit_Best_Params)\n",
    "\t* [Make Predictions Using Classification Grid](#Part_1-Scikit_Predict_Grid)\n",
    "\t* [Calculate Model Prediction Metric Scores: Accuracy, Precision, Recall, and F1](#Part_1-Scikit_Metric_Scores)\n",
    "\t* [Create a Confusion Matrix with the Model Predictions](#Part_1-Scikit_Conf_Mat)\n",
    "\n",
    "\n",
    "* **[Part 2: Model Analysis](#Part_2)**\n",
    "    * [Create Function to Build Keras Neural Net](#Part_2-Build_Keras)\n",
    "\t* [Create TfidfVectorizer and KerasClassifier (Neural Net) Models](#Part_2-Create_Keras_Class)\n",
    "\t* [TfidfVectorize and fit_transform Training and Test Sets](#Part_2-Keras_Vect_Fit)\n",
    "\t* [Setup Parameter Grid and Fit the Pipeline and Parameters to GridSearchCV](#Part_2-Keras_Grid)\n",
    "\t* [Fit TfidfVectorized Training data to Keras Grid to Find the Best Parameters for the Model](#Part_2-Keras_Fit_Tfid)\n",
    "\t* [Display Top Model Scores Found by GridSearchCV](#Part_2-Keras_Top_Scores)\n",
    "\t* [Best Score Found from the Keras Grid](#Part_2-Keras_Best_Score)\n",
    "\t* [Best Parameters Found from the Keras Grid](#Part_2-Keras_Best_Params)\n",
    "\t* [Make Predictions Using the Keras Grid](#Part_2-Keras_Predict)\n",
    "\t* [Calculate Model Prediction Metric Scores: Accuracy, Precision, Recall, and F1](#Part_2-Keras_Metric_Scores)\n",
    "\t* [Create a Confusion Matrix with the Model Predictions](#Part_2-Keras_Conf_Mat)\n",
    "\n",
    "\n",
    "* **[Part 3: Classifying Images](#Part_3)**\n",
    "    * [Implement and Run Keras Image Classification Model](#Part_3-Keras_Image_Class)\n",
    "\t* [Display Accuracy and Loss scores from Each Epoch of the Keras Model](#Part_3-Keras_Acc_Loss_Scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Dataset'></a>\n",
    "## Dataset - Categorical Comment Texts\n",
    "\n",
    "<ins>Columns:</ins><a id='Columns'></a>\n",
    "* __cat__ – Topical category for the comment text (Sports, Video Games, etc...)\n",
    "* __txt__ – Comment text\n",
    "\n",
    "<a id='Imports'></a>\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# NLTK Libraries\n",
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Sklearn Libraries\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.metrics import precision_score, f1_score, accuracy_score, recall_score, confusion_matrix\n",
    "\n",
    "# Keras Libraries\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Classes'></a>\n",
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextNormalizer():\n",
    "    def __init__(self, data, stop_words):\n",
    "        self.data = data\n",
    "        self.stop_words = stop_words\n",
    "        self.porter = PorterStemmer()\n",
    "    \n",
    "    def lower_txt(self, data):\n",
    "        return data.apply(lambda s: s.lower() if type(s) == str else s)\n",
    "        \n",
    "    def remove_punct(self, data):\n",
    "        punctuation = dict.fromkeys(i for i in range(sys.maxunicode) if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "        return data.apply(lambda s: str(s).translate(punctuation))\n",
    "    \n",
    "    def remove_stopword(self, data):\n",
    "        return [' '.join([word for word in s.split()\n",
    "                          if word not in self.stop_words])\n",
    "                for s in data]\n",
    "    \n",
    "    def word_stem(self, data):\n",
    "        return data.apply(lambda word: self.porter.stem(str(word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clean_Data'></a>\n",
    "## Load and Clean Comment Data\n",
    "\n",
    "Shuffle the data randomly and select the first 3,000 rows to reduce the data size and speed up analysis (the neural net takes awhile to train with large sets of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>txt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sports</td>\n",
       "      <td>True, but their top 3 is stacked. \\n\\nCouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>video_games</td>\n",
       "      <td>I had the most fun in halo in custom games</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>video_games</td>\n",
       "      <td>[removed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>video_games</td>\n",
       "      <td>So, whats the point of pushing the payload now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sports</td>\n",
       "      <td>His normal card is already OP  so this one is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           cat                                                txt\n",
       "0       sports  True, but their top 3 is stacked. \\n\\nCouldn't...\n",
       "1  video_games        I had the most fun in halo in custom games \n",
       "2  video_games                                          [removed]\n",
       "3  video_games    So, whats the point of pushing the payload now?\n",
       "4       sports  His normal card is already OP  so this one is ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "commentData = pd.read_json('Comment_Data/categorized-comments.jsonl', lines=True)\n",
    "\n",
    "seed = 34\n",
    "commentData = commentData.sample(frac=1, random_state=seed)[:3000].reset_index(drop=True)\n",
    "\n",
    "commentData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the data is: 3,000 rows and 2 columns\n"
     ]
    }
   ],
   "source": [
    "print('The shape of the data is: {:,} rows and {:,} columns'.format(commentData.shape[0], commentData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Clean_Text'></a>\n",
    "### Clean Text Data: Lower, Remove Punctuation, Remove Stopwords and Stem Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "txtNorm = TextNormalizer(commentData['txt'], stop_words)\n",
    "\n",
    "commentData['lower'] = txtNorm.lower_txt(txtNorm.data)\n",
    "commentData['no_punc'] = txtNorm.remove_punct(commentData['lower'])\n",
    "commentData['rem_stopwords'] = txtNorm.remove_stopword(commentData['no_punc'])\n",
    "commentData['word_stem'] = txtNorm.word_stem(commentData['rem_stopwords'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Split_Dataset'></a>\n",
    "### Split Data Into Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(commentData['word_stem'],\n",
    "                                                    commentData['cat'],\n",
    "                                                    train_size=0.70,\n",
    "                                                    random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Train_Set'></a>\n",
    "#### Training Set: Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988     ok well wont ever agree pcconsole thing bother...\n",
       "1788    ive able technically replace iphone battery it...\n",
       "1782                           dunno timestamp works 2100\n",
       "2410    nooooooooooooooooooooooooooooooooooooooooooooo...\n",
       "1540                             blizzard whimsical today\n",
       "Name: word_stem, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Test_Set'></a>\n",
    "#### Training Set: Target Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "988     video_games\n",
       "1788    video_games\n",
       "1782    video_games\n",
       "2410         sports\n",
       "1540    video_games\n",
       "Name: cat, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1'></a>\n",
    "# Part 1: Neural Network Classifier with Scikit\n",
    "\n",
    "Fit a neural network classifier using scikit-learn, report: accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "<a id='Part_1-Scikit_Pipe'></a>\n",
    "## Create Model Classification Pipeline with TfidfVectorizer and MLPClassifier (Neural Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Pipeline(steps=([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MLPClassifier(hidden_layer_sizes=(6, 2), random_state=seed))\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Grid'></a>\n",
    "## Setup Parameter Grid and Fit Pipeline and Parameters to GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'clf__activation': ['identity', 'logistic', 'tanh', 'relu'],\n",
    "              'clf__learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "              'clf__solver': ['lbfgs', 'sgd', 'adam'],\n",
    "              'clf__alpha': [1e-4, 1e-5, 1e-6]\n",
    "             }\n",
    "\n",
    "class_grid = GridSearchCV(classifier, param_grid = param_grid, cv = 5, n_jobs = -1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Fit_Grid'></a>\n",
    "## Fit Training Data to Classification Grid to Find the Best Parameters for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:   25.1s\n",
      "[Parallel(n_jobs=-1)]: Done 146 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 349 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=-1)]: Done 540 out of 540 | elapsed:  8.0min finished\n",
      "c:\\users\\digital storm\\pycharmprojects\\venv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:582: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('clf',\n",
       "                                        MLPClassifier(hidden_layer_sizes=(6, 2),\n",
       "                                                      random_state=34))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'clf__activation': ['identity', 'logistic', 'tanh',\n",
       "                                             'relu'],\n",
       "                         'clf__alpha': [0.0001, 1e-05, 1e-06],\n",
       "                         'clf__learning_rate': ['constant', 'invscaling',\n",
       "                                                'adaptive'],\n",
       "                         'clf__solver': ['lbfgs', 'sgd', 'adam']},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Top_Scores'></a>\n",
    "## Display Top Model Scores Found by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>{'clf__activation': 'logistic', 'clf__alpha': ...</td>\n",
       "      <td>0.734762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'clf__activation': 'identity', 'clf__alpha': ...</td>\n",
       "      <td>0.730952</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               params  mean_test_score\n",
       "53  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "50  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "32  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "35  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "38  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "41  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "44  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "47  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "29  {'clf__activation': 'logistic', 'clf__alpha': ...         0.734762\n",
       "5   {'clf__activation': 'identity', 'clf__alpha': ...         0.730952"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(class_grid.cv_results_).sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Best_Scores'></a>\n",
    "## Best Score Found From Classification Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7348"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(class_grid.best_score_, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Best_Params'></a>\n",
    "## Best Parameters Found from Classification Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__activation': 'logistic',\n",
       " 'clf__alpha': 0.0001,\n",
       " 'clf__learning_rate': 'constant',\n",
       " 'clf__solver': 'adam'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Predict_Grid'></a>\n",
    "## Make Predictions Using Classification Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = class_grid.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Metric_Scores'></a>\n",
    "## Calculate Model Prediction Metric Scores: Accuracy, Precision, Recall, and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Classification Metric Scores\n",
      "==================================\n",
      "\t Accuracy: 75.00%\n",
      "\tPrecision: 69.47%\n",
      "\t   Recall: 75.00%\n",
      "\t       F1: 70.75%\n"
     ]
    }
   ],
   "source": [
    "accScore = round(accuracy_score(y_test, y_pred), 4) * 100\n",
    "precScore = round(precision_score(y_test, y_pred, average='weighted', zero_division=0), 4) * 100\n",
    "recallScore = round(recall_score(y_test, y_pred, average='weighted'), 4) * 100\n",
    "f1_Score = round(f1_score(y_test, y_pred, average='weighted'), 4) * 100\n",
    "\n",
    "print('Model Classification Metric Scores\\n' + '='*34 + \n",
    "      '\\n\\t Accuracy: {:,.2f}%\\n\\tPrecision: {:,.2f}%\\n\\t   Recall: {:,.2f}%\\n\\t       F1: {:,.2f}%'\n",
    "      .format(accScore, precScore, recallScore, f1_Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_1-Scikit_Conf_Mat'></a>\n",
    "## Create a Confusion Matrix with the Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,  10,  26],\n",
       "       [  0,  63, 149],\n",
       "       [  0,  40, 612]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2'></a>\n",
    "# Part 2: Neural Network Classifier with Keras\n",
    "\n",
    "Fit a neural network classifier using Keras, report: accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "<a id='Part_2-Build_Keras'></a>\n",
    "## Create Function to Build Keras Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = x_train.shape[0]\n",
    "n_classes = len(y_train.unique())\n",
    "\n",
    "def build_network():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(250, activation='relu', input_dim=(n_features)))\n",
    "    nn.add(Dense(100, activation='relu'))\n",
    "    nn.add(Dense(n_classes, activation='softmax'))\n",
    "    \n",
    "    nn.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Create_Keras_Class'></a>\n",
    "## Create TfidfVectorizer and KerasClassifier (Neural Net) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(max_features=n_features)\n",
    "\n",
    "keras_model = KerasClassifier(build_fn=build_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Vect_Fit'></a>\n",
    "## TfidfVectorize and fit_transform Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_vect = vect.fit_transform(x_train).todense()\n",
    "x_test_vect = vect.fit_transform(x_test).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Grid'></a>\n",
    "## Setup Parameter Grid and Fit the Pipeline and Parameters to GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {'epochs': [10, 25, 50],\n",
    "              'batch_size': [10, 25, 50]}\n",
    "\n",
    "keras_grid = GridSearchCV(keras_model, param_grid = param_grid, cv = 5, n_jobs = -1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Fit_Tfid'></a>\n",
    "## Fit TfidfVectorized Training data to Keras Grid to Find the Best Parameters for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:  8.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.7902 - accuracy: 0.7062\n",
      "Epoch 2/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.5812 - accuracy: 0.7205\n",
      "Epoch 3/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.3956 - accuracy: 0.8424\n",
      "Epoch 4/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.2500 - accuracy: 0.9038\n",
      "Epoch 5/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1837 - accuracy: 0.9238\n",
      "Epoch 6/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.1454 - accuracy: 0.9448\n",
      "Epoch 7/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9619\n",
      "Epoch 8/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.1052 - accuracy: 0.9614\n",
      "Epoch 9/10\n",
      "42/42 [==============================] - 0s 8ms/step - loss: 0.0971 - accuracy: 0.9629\n",
      "Epoch 10/10\n",
      "42/42 [==============================] - 0s 9ms/step - loss: 0.0957 - accuracy: 0.9595\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x000001B43076E040>,\n",
       "             n_jobs=-1,\n",
       "             param_grid={'batch_size': [10, 25, 50], 'epochs': [10, 25, 50]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_grid.fit(x_train_vect, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Top_Scores'></a>\n",
    "## Display Top Model Scores Found by GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>{'batch_size': 50, 'epochs': 10}</td>\n",
       "      <td>0.730952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'batch_size': 25, 'epochs': 10}</td>\n",
       "      <td>0.730952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>{'batch_size': 50, 'epochs': 50}</td>\n",
       "      <td>0.727143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'batch_size': 10, 'epochs': 10}</td>\n",
       "      <td>0.726667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'batch_size': 10, 'epochs': 25}</td>\n",
       "      <td>0.725238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'batch_size': 25, 'epochs': 25}</td>\n",
       "      <td>0.722381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>{'batch_size': 25, 'epochs': 50}</td>\n",
       "      <td>0.721905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>{'batch_size': 50, 'epochs': 25}</td>\n",
       "      <td>0.716190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'batch_size': 10, 'epochs': 50}</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             params  mean_test_score\n",
       "6  {'batch_size': 50, 'epochs': 10}         0.730952\n",
       "3  {'batch_size': 25, 'epochs': 10}         0.730952\n",
       "8  {'batch_size': 50, 'epochs': 50}         0.727143\n",
       "0  {'batch_size': 10, 'epochs': 10}         0.726667\n",
       "1  {'batch_size': 10, 'epochs': 25}         0.725238\n",
       "4  {'batch_size': 25, 'epochs': 25}         0.722381\n",
       "5  {'batch_size': 25, 'epochs': 50}         0.721905\n",
       "7  {'batch_size': 50, 'epochs': 25}         0.716190\n",
       "2  {'batch_size': 10, 'epochs': 50}         0.714286"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(keras_grid.cv_results_).sort_values('mean_test_score', ascending=False)[['params', 'mean_test_score']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Best_Score'></a>\n",
    "## Best Score Found from the Keras Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.731"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(keras_grid.best_score_, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Best_Params'></a>\n",
    "## Best Parameters Found from the Keras Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 50, 'epochs': 10}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras_grid.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Predict'></a>\n",
    "## Make Predictions Using the Keras Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = keras_grid.predict(x_test_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Metric_Scores'></a>\n",
    "## Calculate Model Prediction Metric Scores: Accuracy, Precision, Recall, and F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Keras Metric Scores\n",
      "=========================\n",
      "     Accuracy: 73.44%\n",
      "    Precision: 69.94%\n",
      "       Recall: 73.44%\n",
      "\t   F1: 69.84%\n"
     ]
    }
   ],
   "source": [
    "accScore = round(accuracy_score(y_test, y_pred), 4) * 100\n",
    "precScore = round(precision_score(y_test, y_pred, average='weighted', zero_division=0), 4) * 100\n",
    "recallScore = round(recall_score(y_test, y_pred, average='weighted'), 4) * 100\n",
    "f1_Score = round(f1_score(y_test, y_pred, average='weighted'), 4) * 100\n",
    "\n",
    "print('Model Keras Metric Scores\\n' + '='*25 + \n",
    "      '\\n     Accuracy: {:,.2f}%\\n    Precision: {:,.2f}%\\n       Recall: {:,.2f}%\\n\\t   F1: {:,.2f}%'\n",
    "      .format(accScore, precScore, recallScore, f1_Score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_2-Keras_Conf_Mat'></a>\n",
    "## Create a Confusion Matrix with the Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  3,   4,  29],\n",
       "       [  0,  61, 151],\n",
       "       [  3,  52, 597]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_3'></a>\n",
    "# Part 3: Classifying Images\n",
    "\n",
    "Classify MSINT images using a convolutional neural network and report the accuracy of the results.\n",
    "\n",
    "<a id='Part_3-Keras_Image_Class'></a>\n",
    "## Implement and Run Keras Image Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "60/60 - 29s - loss: 0.6155 - accuracy: 0.8062 - val_loss: 0.1588 - val_accuracy: 0.9541\n",
      "Epoch 2/2\n",
      "60/60 - 28s - loss: 0.1915 - accuracy: 0.9442 - val_loss: 0.0899 - val_accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "# Set that the color channel value will be first\n",
    "K.set_image_data_format(\"channels_last\")\n",
    "\n",
    "# Set seed\n",
    "np.random.seed(0)\n",
    "\n",
    "# Set image information\n",
    "channels = 1\n",
    "height = 28\n",
    "width = 28\n",
    "\n",
    "# Load data and target from MNIST data\n",
    "(data_train, target_train), (data_test, target_test) = mnist.load_data()\n",
    "                                                                       \n",
    "# Reshape training image data into features\n",
    "data_train = data_train.reshape(data_train.shape[0], height, width, channels)\n",
    "\n",
    "# Reshape test image data into features\n",
    "data_test = data_test.reshape(data_test.shape[0], height, width, channels)\n",
    "\n",
    "# Rescale pixel intensity to between 0 and 1\n",
    "features_train = data_train / 255\n",
    "features_test = data_test / 255\n",
    "\n",
    "# One-hot encode target\n",
    "target_train = np_utils.to_categorical(target_train)\n",
    "target_test = np_utils.to_categorical(target_test)\n",
    "number_of_classes = target_test.shape[1]\n",
    "\n",
    "# Start neural network\n",
    "network = Sequential()\n",
    "\n",
    "# Add convolutional layer with 64 filters, a 5x5 window, and ReLU activation function\n",
    "network.add(Conv2D(filters=64, kernel_size=(5, 5), kernel_initializer='normal', padding='valid',\n",
    "                   input_shape=(width, height, channels), activation='relu'))\n",
    "\n",
    "# Add max pooling layer with a 2x2 window\n",
    "network.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add layer to flatten input\n",
    "network.add(Flatten())\n",
    "\n",
    "# # Add fully connected layer of 128 units with a ReLU activation function\n",
    "network.add(Dense(128, activation=\"relu\"))\n",
    "\n",
    "# Add dropout layer\n",
    "network.add(Dropout(0.5))\n",
    "\n",
    "# Add fully connected layer with a softmax activation function\n",
    "network.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "\n",
    "# Compile neural network\n",
    "network.compile(loss=\"categorical_crossentropy\", # Cross-entropy\n",
    "                optimizer=\"rmsprop\", # Root Mean Square Propagation\n",
    "                metrics=[\"accuracy\"]) # Accuracy performance metric\n",
    "\n",
    "# Train neural network\n",
    "history = network.fit(features_train, # Features\n",
    "                      target_train, # Target\n",
    "                      epochs=2, # Number of epochs\n",
    "                      verbose=2, # Print description after each epoch\n",
    "                      batch_size=1000, # Number of observations per batch\n",
    "                      validation_data=(features_test, target_test)) # Data for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Part_3-Keras_Acc_Loss_Scores'></a>\n",
    "## Display Accuracy and Loss scores from Each Epoch of the Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Keras Image Classification Accuracies\n",
      "=========================================\n",
      "Epoch 1: Accuracy = 80.62%, Loss = 61.55%\n",
      "Epoch 2: Accuracy = 94.42%, Loss = 19.15%\n"
     ]
    }
   ],
   "source": [
    "print('  Keras Image Classification Accuracies\\n' + '='*41)\n",
    "\n",
    "for index, (acc, loss) in enumerate(zip(history.history['accuracy'], history.history['loss'])):\n",
    "    print('Epoch {:}: Accuracy = {:,.2f}%, Loss = {:,.2f}%'.format(index + 1, acc * 100, loss * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
